{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ifyuc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ifyuc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "nltk.download(\"stopwords\")\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the news text you want to verify: President Trump was banned from Twitter after he posted a series of tweets deemed to be in violation of Twitter's rules on inciting violence. He tweeted almost daily during his presidency and saw it as a way to communicate directly with his nearly 90 million followers.\n",
      "You entered: President Trump was banned from Twitter after he posted a series of tweets deemed to be in violation of Twitter's rules on inciting violence. He tweeted almost daily during his presidency and saw it as a way to communicate directly with his nearly 90 million followers.\n"
     ]
    }
   ],
   "source": [
    "var = input(\"Please enter the news text you want to verify: \")\n",
    "print(\"You entered: \" + str(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var = remove_stopwords(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['president',\n",
       " 'trump',\n",
       " 'banned',\n",
       " 'twitter',\n",
       " 'posted',\n",
       " 'series',\n",
       " 'tweets',\n",
       " 'deemed',\n",
       " 'violation',\n",
       " 'twitter',\n",
       " 'rules',\n",
       " 'inciting',\n",
       " 'violence',\n",
       " 'tweeted',\n",
       " 'daily',\n",
       " 'presidency',\n",
       " 'communicate',\n",
       " 'directly',\n",
       " 'nearly',\n",
       " 'million',\n",
       " 'followers']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "for i in new_var:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(list(set(list_of_words)))\n",
    "total_words #unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_var=\" \".join(list(new_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'president trump banned twitter posted series tweets deemed violation twitter rules inciting violence tweeted daily presidency communicate directly nearly million followers'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from tensorflow.keras.models import load_model\\n\\n#function to run for prediction\\ndef output(var):    \\n#retrieving the best model for prediction call\\n    new_model = load_model(\\'models/lstm_model.h5\\')\\n    prediction = new_model.predict([var])\\n    prob = new_model.predict_proba([var])\\n\\n    return (print(\"The given statement is \",prediction[0]),\\n        print(\"The truth probability score is \",prob[0][1]))\\n\\noutput(var)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from tensorflow.keras.models import load_model\n",
    "\n",
    "#function to run for prediction\n",
    "def output(var):    \n",
    "#retrieving the best model for prediction call\n",
    "    new_model = load_model('models/lstm_model.h5')\n",
    "    prediction = new_model.predict([var])\n",
    "    prob = new_model.predict_proba([var])\n",
    "\n",
    "    return (print(\"The given statement is \",prediction[0]),\n",
    "        print(\"The truth probability score is \",prob[0][1]))\n",
    "\n",
    "output(var)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xtest = tokenizer.texts_to_sequences([var])\\nxtest = sequence.pad_sequences(var, maxlen=maxlen)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''xtest = tokenizer.texts_to_sequences([var])\n",
    "xtest = sequence.pad_sequences(var, maxlen=maxlen)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textin sahip oldugu kelime sayisi 21\n"
     ]
    }
   ],
   "source": [
    "maxlen = -1\n",
    "tokens = nltk.word_tokenize(new_var)\n",
    "if(maxlen<len(tokens)):\n",
    "    maxlen = len(tokens)\n",
    "print(\"textin sahip oldugu kelime sayisi\", maxlen) #word embeding icin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenizer = Tokenizer(num_words = total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts([new_var])\n",
    "test_sequences = tokenizer.texts_to_sequences([new_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 4, 1, 5, 6, 7, 8, 9, 1, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_test = pad_sequences(test_sequences,maxlen = maxlen,padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  1,  5,  6,  7,  8,  9,  1, 10, 11, 12, 13, 14, 15,\n",
       "        16, 17, 18, 19,  0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52072835]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(pred):\n",
    "    if pred[0][0] > 0.50:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
