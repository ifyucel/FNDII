{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.4.1\n",
      "Keras Version: 2.4.0\n",
      "\n",
      "Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "Pandas 1.1.3\n",
      "Scikit-Learn 0.23.2\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ifyuc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18285 entries, 0 to 20799\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      18285 non-null  int64 \n",
      " 1   title   18285 non-null  object\n",
      " 2   author  18285 non-null  object\n",
      " 3   text    18285 non-null  object\n",
      " 4   label   18285 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 857.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "df = shuffle(df)\n",
    "df = df.reset_index(drop=True)\n",
    "df.isnull().sum()    \n",
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"conversion_dict = { 0:'Real', 1:'Fake'}\\ndf['label'] = df['label'].replace(conversion_dict)\\ndf.label.value_counts()\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''conversion_dict = { 0:'Real', 1:'Fake'}\n",
    "df['label'] = df['label'].replace(conversion_dict)\n",
    "df.label.value_counts()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"title\"],axis=1,inplace=True)\n",
    "df.drop([\"id\"],axis=1,inplace=True)\n",
    "df.drop([\"author\"],axis=1,inplace=True)\n",
    "\n",
    "df['text']=df['text'].apply(str)\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "\n",
    "df = shuffle(df)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  national highway traffic safety administration in the usa today gave clarification on the soon to be imposed safety measures required for electric cars. in addition to a two-tone siren that must sound the word “pus-sy!” and a yellow strobe light mounted on the roof, all owners must carry a man who will walk in front of the car holding a red flag. \\n‘the nhtsa has worked long and hard on these proposals with our colleagues in detroit,’ a spokesman – george mustangveight – said. ‘over one americans every year are injured by electric cars and we aim to resolve this unacceptable risk to our citizens with these actions. plus, the forced employment of a pedestrian speed retarder by all owners of these dangerous vehicles will do wonders for the unemployment these blights have caused in our oil and petrochemical industries.’ \\nin california, tesla motors owner elon musk remained positive: ‘it’s fine. i’m currently working on a way to convert electricity using the phone lines as a transport medium. i’m sure we can have it running in beta by 2020. failing that we may soon be able to transubstantiate the intolerable smugness of their owners into pure energy’. james_doc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ifyuc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 and 1 letter words delete and return list\n",
    "def remove_stopwords(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['national',\n",
       " 'highway',\n",
       " 'traffic',\n",
       " 'safety',\n",
       " 'administration',\n",
       " 'today',\n",
       " 'gave',\n",
       " 'clarification',\n",
       " 'soon',\n",
       " 'imposed',\n",
       " 'safety',\n",
       " 'measures',\n",
       " 'required',\n",
       " 'electric',\n",
       " 'cars',\n",
       " 'addition',\n",
       " 'tone',\n",
       " 'siren',\n",
       " 'sound',\n",
       " 'word',\n",
       " 'yellow',\n",
       " 'strobe',\n",
       " 'light',\n",
       " 'mounted',\n",
       " 'roof',\n",
       " 'owners',\n",
       " 'carry',\n",
       " 'walk',\n",
       " 'holding',\n",
       " 'flag',\n",
       " 'nhtsa',\n",
       " 'worked',\n",
       " 'long',\n",
       " 'hard',\n",
       " 'proposals',\n",
       " 'colleagues',\n",
       " 'detroit',\n",
       " 'spokesman',\n",
       " 'george',\n",
       " 'mustangveight',\n",
       " 'said',\n",
       " 'americans',\n",
       " 'year',\n",
       " 'injured',\n",
       " 'electric',\n",
       " 'cars',\n",
       " 'resolve',\n",
       " 'unacceptable',\n",
       " 'risk',\n",
       " 'citizens',\n",
       " 'actions',\n",
       " 'plus',\n",
       " 'forced',\n",
       " 'employment',\n",
       " 'pedestrian',\n",
       " 'speed',\n",
       " 'retarder',\n",
       " 'owners',\n",
       " 'dangerous',\n",
       " 'vehicles',\n",
       " 'wonders',\n",
       " 'unemployment',\n",
       " 'blights',\n",
       " 'caused',\n",
       " 'petrochemical',\n",
       " 'industries',\n",
       " 'california',\n",
       " 'tesla',\n",
       " 'motors',\n",
       " 'owner',\n",
       " 'elon',\n",
       " 'musk',\n",
       " 'remained',\n",
       " 'positive',\n",
       " 'fine',\n",
       " 'currently',\n",
       " 'working',\n",
       " 'convert',\n",
       " 'electricity',\n",
       " 'phone',\n",
       " 'lines',\n",
       " 'transport',\n",
       " 'medium',\n",
       " 'sure',\n",
       " 'running',\n",
       " 'beta',\n",
       " 'failing',\n",
       " 'soon',\n",
       " 'able',\n",
       " 'intolerable',\n",
       " 'smugness',\n",
       " 'owners',\n",
       " 'pure',\n",
       " 'energy',\n",
       " 'james_doc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[national, highway, traffic, safety, administr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[want, support, receive, access, tons, bonus, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[kabul, afghanistan, months, failed, pakistani...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tatiana, ricardo, single, mother, children, i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tuesday, news, channel, hannity, mark, meadow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18280</th>\n",
       "      <td>[janeiro, famous, athletes, world, like, muham...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18281</th>\n",
       "      <td>[david, duke, prof, kevin, macdonald, duke, ov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>[rufus, farmer, tired, ways, black, mistreated...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18283</th>\n",
       "      <td>[powerful, array, republican, party, largest, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18284</th>\n",
       "      <td>[june, sped, crowded, subway, people, street, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18285 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      [national, highway, traffic, safety, administr...      1\n",
       "1      [want, support, receive, access, tons, bonus, ...      1\n",
       "2      [kabul, afghanistan, months, failed, pakistani...      0\n",
       "3      [tatiana, ricardo, single, mother, children, i...      0\n",
       "4      [tuesday, news, channel, hannity, mark, meadow...      0\n",
       "...                                                  ...    ...\n",
       "18280  [janeiro, famous, athletes, world, like, muham...      0\n",
       "18281  [david, duke, prof, kevin, macdonald, duke, ov...      1\n",
       "18282  [rufus, farmer, tired, ways, black, mistreated...      0\n",
       "18283  [powerful, array, republican, party, largest, ...      0\n",
       "18284  [june, sped, crowded, subway, people, street, ...      0\n",
       "\n",
       "[18285 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "for i in df.text:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6776215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149278"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words = len(list(set(list_of_words)))\n",
    "total_words #unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_of_use'] = df['text'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'national highway traffic safety administration today gave clarification soon imposed safety measures required electric cars addition tone siren sound word yellow strobe light mounted roof owners carry walk holding flag nhtsa worked long hard proposals colleagues detroit spokesman george mustangveight said americans year injured electric cars resolve unacceptable risk citizens actions plus forced employment pedestrian speed retarder owners dangerous vehicles wonders unemployment blights caused petrochemical industries california tesla motors owner elon musk remained positive fine currently working convert electricity phone lines transport medium sure running beta failing soon able intolerable smugness owners pure energy james_doc'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_of_use'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['national',\n",
       " 'highway',\n",
       " 'traffic',\n",
       " 'safety',\n",
       " 'administration',\n",
       " 'today',\n",
       " 'gave',\n",
       " 'clarification',\n",
       " 'soon',\n",
       " 'imposed',\n",
       " 'safety',\n",
       " 'measures',\n",
       " 'required',\n",
       " 'electric',\n",
       " 'cars',\n",
       " 'addition',\n",
       " 'tone',\n",
       " 'siren',\n",
       " 'sound',\n",
       " 'word',\n",
       " 'yellow',\n",
       " 'strobe',\n",
       " 'light',\n",
       " 'mounted',\n",
       " 'roof',\n",
       " 'owners',\n",
       " 'carry',\n",
       " 'walk',\n",
       " 'holding',\n",
       " 'flag',\n",
       " 'nhtsa',\n",
       " 'worked',\n",
       " 'long',\n",
       " 'hard',\n",
       " 'proposals',\n",
       " 'colleagues',\n",
       " 'detroit',\n",
       " 'spokesman',\n",
       " 'george',\n",
       " 'mustangveight',\n",
       " 'said',\n",
       " 'americans',\n",
       " 'year',\n",
       " 'injured',\n",
       " 'electric',\n",
       " 'cars',\n",
       " 'resolve',\n",
       " 'unacceptable',\n",
       " 'risk',\n",
       " 'citizens',\n",
       " 'actions',\n",
       " 'plus',\n",
       " 'forced',\n",
       " 'employment',\n",
       " 'pedestrian',\n",
       " 'speed',\n",
       " 'retarder',\n",
       " 'owners',\n",
       " 'dangerous',\n",
       " 'vehicles',\n",
       " 'wonders',\n",
       " 'unemployment',\n",
       " 'blights',\n",
       " 'caused',\n",
       " 'petrochemical',\n",
       " 'industries',\n",
       " 'california',\n",
       " 'tesla',\n",
       " 'motors',\n",
       " 'owner',\n",
       " 'elon',\n",
       " 'musk',\n",
       " 'remained',\n",
       " 'positive',\n",
       " 'fine',\n",
       " 'currently',\n",
       " 'working',\n",
       " 'convert',\n",
       " 'electricity',\n",
       " 'phone',\n",
       " 'lines',\n",
       " 'transport',\n",
       " 'medium',\n",
       " 'sure',\n",
       " 'running',\n",
       " 'beta',\n",
       " 'failing',\n",
       " 'soon',\n",
       " 'able',\n",
       " 'intolerable',\n",
       " 'smugness',\n",
       " 'owners',\n",
       " 'pure',\n",
       " 'energy',\n",
       " 'james_doc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_of_use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[national, highway, traffic, safety, administr...</td>\n",
       "      <td>1</td>\n",
       "      <td>national highway traffic safety administration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[want, support, receive, access, tons, bonus, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>want support receive access tons bonus content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[kabul, afghanistan, months, failed, pakistani...</td>\n",
       "      <td>0</td>\n",
       "      <td>kabul afghanistan months failed pakistani effo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tatiana, ricardo, single, mother, children, i...</td>\n",
       "      <td>0</td>\n",
       "      <td>tatiana ricardo single mother children includi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tuesday, news, channel, hannity, mark, meadow...</td>\n",
       "      <td>0</td>\n",
       "      <td>tuesday news channel hannity mark meadows said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18280</th>\n",
       "      <td>[janeiro, famous, athletes, world, like, muham...</td>\n",
       "      <td>0</td>\n",
       "      <td>janeiro famous athletes world like muhammad ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18281</th>\n",
       "      <td>[david, duke, prof, kevin, macdonald, duke, ov...</td>\n",
       "      <td>1</td>\n",
       "      <td>david duke prof kevin macdonald duke overwhelm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18282</th>\n",
       "      <td>[rufus, farmer, tired, ways, black, mistreated...</td>\n",
       "      <td>0</td>\n",
       "      <td>rufus farmer tired ways black mistreated natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18283</th>\n",
       "      <td>[powerful, array, republican, party, largest, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>powerful array republican party largest financ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18284</th>\n",
       "      <td>[june, sped, crowded, subway, people, street, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>june sped crowded subway people street stopped...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18285 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      [national, highway, traffic, safety, administr...      1   \n",
       "1      [want, support, receive, access, tons, bonus, ...      1   \n",
       "2      [kabul, afghanistan, months, failed, pakistani...      0   \n",
       "3      [tatiana, ricardo, single, mother, children, i...      0   \n",
       "4      [tuesday, news, channel, hannity, mark, meadow...      0   \n",
       "...                                                  ...    ...   \n",
       "18280  [janeiro, famous, athletes, world, like, muham...      0   \n",
       "18281  [david, duke, prof, kevin, macdonald, duke, ov...      1   \n",
       "18282  [rufus, farmer, tired, ways, black, mistreated...      0   \n",
       "18283  [powerful, array, republican, party, largest, ...      0   \n",
       "18284  [june, sped, crowded, subway, people, street, ...      0   \n",
       "\n",
       "                                             text_of_use  \n",
       "0      national highway traffic safety administration...  \n",
       "1      want support receive access tons bonus content...  \n",
       "2      kabul afghanistan months failed pakistani effo...  \n",
       "3      tatiana ricardo single mother children includi...  \n",
       "4      tuesday news channel hannity mark meadows said...  \n",
       "...                                                  ...  \n",
       "18280  janeiro famous athletes world like muhammad ti...  \n",
       "18281  david duke prof kevin macdonald duke overwhelm...  \n",
       "18282  rufus farmer tired ways black mistreated natio...  \n",
       "18283  powerful array republican party largest financ...  \n",
       "18284  june sped crowded subway people street stopped...  \n",
       "\n",
       "[18285 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text of use da en cok kelimeye sahip single row su kadar kelimeye sahip 13682\n"
     ]
    }
   ],
   "source": [
    "maxlen = -1\n",
    "for doc in df.text_of_use:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen<len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"text of use da en cok kelimeye sahip single row su kadar kelimeye sahip\", maxlen) #word embeding icin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.text_of_use, df.label, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = total_words) #tokenizer create to token unique words\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "train_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70,\n",
       " 2914,\n",
       " 1511,\n",
       " 17,\n",
       " 211,\n",
       " 418,\n",
       " 13143,\n",
       " 2181,\n",
       " 470,\n",
       " 1825,\n",
       " 438,\n",
       " 11988,\n",
       " 3030,\n",
       " 457,\n",
       " 10540,\n",
       " 5566,\n",
       " 3297,\n",
       " 8481,\n",
       " 133,\n",
       " 211,\n",
       " 418,\n",
       " 1680,\n",
       " 4289,\n",
       " 3244,\n",
       " 11864,\n",
       " 17638,\n",
       " 406,\n",
       " 1230,\n",
       " 247,\n",
       " 19859,\n",
       " 30465,\n",
       " 46077,\n",
       " 416,\n",
       " 780,\n",
       " 12805,\n",
       " 2945,\n",
       " 6327,\n",
       " 28337,\n",
       " 17483,\n",
       " 406,\n",
       " 165,\n",
       " 432,\n",
       " 25976,\n",
       " 406,\n",
       " 1230,\n",
       " 5965,\n",
       " 28337,\n",
       " 17483,\n",
       " 7412,\n",
       " 1891,\n",
       " 4395,\n",
       " 32810,\n",
       " 13143,\n",
       " 2181,\n",
       " 470,\n",
       " 1825,\n",
       " 438,\n",
       " 11988,\n",
       " 3030,\n",
       " 457,\n",
       " 10540,\n",
       " 5566,\n",
       " 3297,\n",
       " 8481,\n",
       " 133,\n",
       " 359,\n",
       " 1259,\n",
       " 105,\n",
       " 15370,\n",
       " 4,\n",
       " 1717,\n",
       " 12429,\n",
       " 65,\n",
       " 3981,\n",
       " 11245,\n",
       " 1280,\n",
       " 922,\n",
       " 1245,\n",
       " 4822,\n",
       " 161,\n",
       " 3395,\n",
       " 2474,\n",
       " 2681,\n",
       " 21048,\n",
       " 3285,\n",
       " 5322,\n",
       " 4339,\n",
       " 239,\n",
       " 9048,\n",
       " 6371,\n",
       " 2126,\n",
       " 150,\n",
       " 30144,\n",
       " 753,\n",
       " 11145,\n",
       " 64,\n",
       " 13143,\n",
       " 2181,\n",
       " 358,\n",
       " 2729,\n",
       " 61,\n",
       " 450,\n",
       " 498,\n",
       " 6726,\n",
       " 13143,\n",
       " 1055,\n",
       " 11,\n",
       " 954,\n",
       " 752,\n",
       " 3825,\n",
       " 752,\n",
       " 740,\n",
       " 203,\n",
       " 522,\n",
       " 13143,\n",
       " 638,\n",
       " 878,\n",
       " 96,\n",
       " 409,\n",
       " 4728,\n",
       " 578,\n",
       " 1977,\n",
       " 11,\n",
       " 4784,\n",
       " 96,\n",
       " 954,\n",
       " 37487,\n",
       " 1160,\n",
       " 5896,\n",
       " 813,\n",
       " 4867,\n",
       " 1598,\n",
       " 4784,\n",
       " 2711,\n",
       " 924,\n",
       " 80,\n",
       " 3328,\n",
       " 3849,\n",
       " 117,\n",
       " 985,\n",
       " 892,\n",
       " 3223,\n",
       " 1267,\n",
       " 11,\n",
       " 740,\n",
       " 224,\n",
       " 1416,\n",
       " 985,\n",
       " 8308,\n",
       " 3180,\n",
       " 522,\n",
       " 7186,\n",
       " 377,\n",
       " 371,\n",
       " 145,\n",
       " 377,\n",
       " 371,\n",
       " 76,\n",
       " 3659,\n",
       " 523,\n",
       " 3659,\n",
       " 377,\n",
       " 371,\n",
       " 292,\n",
       " 3659,\n",
       " 377,\n",
       " 371,\n",
       " 370,\n",
       " 2284,\n",
       " 13143,\n",
       " 264,\n",
       " 3,\n",
       " 4748,\n",
       " 13143,\n",
       " 20,\n",
       " 13143,\n",
       " 1242,\n",
       " 3868,\n",
       " 300,\n",
       " 1232,\n",
       " 772,\n",
       " 868,\n",
       " 881,\n",
       " 148,\n",
       " 13,\n",
       " 2677,\n",
       " 3,\n",
       " 114,\n",
       " 148,\n",
       " 2677,\n",
       " 412,\n",
       " 150,\n",
       " 20,\n",
       " 13143,\n",
       " 6897,\n",
       " 187,\n",
       " 20,\n",
       " 13143,\n",
       " 150,\n",
       " 444,\n",
       " 5031,\n",
       " 13143,\n",
       " 108,\n",
       " 5216,\n",
       " 6461,\n",
       " 2347,\n",
       " 280,\n",
       " 13143,\n",
       " 20,\n",
       " 752,\n",
       " 13143,\n",
       " 125,\n",
       " 5344,\n",
       " 578,\n",
       " 84,\n",
       " 150,\n",
       " 13143,\n",
       " 19759,\n",
       " 24529,\n",
       " 11130,\n",
       " 418,\n",
       " 2697,\n",
       " 2103,\n",
       " 24529,\n",
       " 309,\n",
       " 4272,\n",
       " 24529,\n",
       " 4735,\n",
       " 866,\n",
       " 742,\n",
       " 2406,\n",
       " 6930,\n",
       " 406,\n",
       " 22471,\n",
       " 2470,\n",
       " 1093,\n",
       " 1001,\n",
       " 5324,\n",
       " 82,\n",
       " 20,\n",
       " 38699,\n",
       " 2513,\n",
       " 3324,\n",
       " 504,\n",
       " 2047,\n",
       " 63,\n",
       " 147,\n",
       " 2246,\n",
       " 2193,\n",
       " 43,\n",
       " 861,\n",
       " 3128,\n",
       " 2170,\n",
       " 76,\n",
       " 6024,\n",
       " 224,\n",
       " 4650,\n",
       " 2103,\n",
       " 6853,\n",
       " 1576,\n",
       " 3139,\n",
       " 25757,\n",
       " 5199,\n",
       " 411,\n",
       " 3704,\n",
       " 1928,\n",
       " 4413,\n",
       " 4683,\n",
       " 4349,\n",
       " 14874,\n",
       " 4413,\n",
       " 4683,\n",
       " 61,\n",
       " 39686,\n",
       " 2684,\n",
       " 3988,\n",
       " 619,\n",
       " 2001,\n",
       " 619,\n",
       " 575,\n",
       " 2450,\n",
       " 2243,\n",
       " 3077,\n",
       " 3398,\n",
       " 10823,\n",
       " 6443,\n",
       " 897,\n",
       " 2243,\n",
       " 35930,\n",
       " 1906,\n",
       " 5083,\n",
       " 4589,\n",
       " 706,\n",
       " 2353,\n",
       " 7983,\n",
       " 881,\n",
       " 5366,\n",
       " 6355,\n",
       " 588,\n",
       " 3974,\n",
       " 3708,\n",
       " 14245,\n",
       " 2001,\n",
       " 12844,\n",
       " 108,\n",
       " 1804,\n",
       " 24529,\n",
       " 2001,\n",
       " 1111,\n",
       " 1804,\n",
       " 1513,\n",
       " 350,\n",
       " 2640,\n",
       " 350,\n",
       " 1721,\n",
       " 8468,\n",
       " 600,\n",
       " 3199,\n",
       " 5854,\n",
       " 619,\n",
       " 2231,\n",
       " 390,\n",
       " 159,\n",
       " 76,\n",
       " 150,\n",
       " 198,\n",
       " 4247,\n",
       " 600,\n",
       " 227,\n",
       " 1804,\n",
       " 1513,\n",
       " 975,\n",
       " 659,\n",
       " 12081,\n",
       " 5311,\n",
       " 167,\n",
       " 286,\n",
       " 792,\n",
       " 7186,\n",
       " 227,\n",
       " 3336,\n",
       " 34109,\n",
       " 659,\n",
       " 868,\n",
       " 12445,\n",
       " 4547,\n",
       " 28436,\n",
       " 5112,\n",
       " 2505,\n",
       " 1002,\n",
       " 2163,\n",
       " 600,\n",
       " 711,\n",
       " 43029,\n",
       " 1696,\n",
       " 3281,\n",
       " 541,\n",
       " 108,\n",
       " 659,\n",
       " 65073,\n",
       " 4041,\n",
       " 287,\n",
       " 3344,\n",
       " 859,\n",
       " 7688,\n",
       " 84054,\n",
       " 11,\n",
       " 6935,\n",
       " 2185,\n",
       " 8749,\n",
       " 2696,\n",
       " 125,\n",
       " 13024,\n",
       " 1648,\n",
       " 461,\n",
       " 298,\n",
       " 1926,\n",
       " 2696,\n",
       " 34709,\n",
       " 1568,\n",
       " 12001,\n",
       " 48017,\n",
       " 4128,\n",
       " 286,\n",
       " 11421,\n",
       " 1972,\n",
       " 3056,\n",
       " 8348,\n",
       " 282,\n",
       " 165,\n",
       " 659,\n",
       " 1851,\n",
       " 6054,\n",
       " 4959,\n",
       " 21872,\n",
       " 2871,\n",
       " 7655,\n",
       " 286,\n",
       " 3791,\n",
       " 564,\n",
       " 3701,\n",
       " 9204,\n",
       " 11320,\n",
       " 11421,\n",
       " 283,\n",
       " 50,\n",
       " 4346,\n",
       " 5746,\n",
       " 2148,\n",
       " 2386,\n",
       " 1735,\n",
       " 12300,\n",
       " 280,\n",
       " 286,\n",
       " 400,\n",
       " 422,\n",
       " 1500,\n",
       " 25138,\n",
       " 6945,\n",
       " 12657,\n",
       " 2051,\n",
       " 2247,\n",
       " 6371,\n",
       " 5039,\n",
       " 9188,\n",
       " 592,\n",
       " 76,\n",
       " 74,\n",
       " 3833,\n",
       " 41,\n",
       " 4805,\n",
       " 21372,\n",
       " 3175,\n",
       " 2148,\n",
       " 98,\n",
       " 3223,\n",
       " 4314,\n",
       " 1710,\n",
       " 3030,\n",
       " 1274,\n",
       " 7460,\n",
       " 3172,\n",
       " 2300,\n",
       " 99,\n",
       " 6677,\n",
       " 13610,\n",
       " 2615,\n",
       " 1366,\n",
       " 1232,\n",
       " 889,\n",
       " 13143,\n",
       " 3503,\n",
       " 13913,\n",
       " 5351,\n",
       " 2513,\n",
       " 2513,\n",
       " 337,\n",
       " 25,\n",
       " 54655,\n",
       " 847,\n",
       " 2513,\n",
       " 3694,\n",
       " 10387,\n",
       " 5291,\n",
       " 659,\n",
       " 51,\n",
       " 749,\n",
       " 2975,\n",
       " 1083,\n",
       " 286,\n",
       " 842,\n",
       " 549,\n",
       " 3310,\n",
       " 2975,\n",
       " 659,\n",
       " 17886,\n",
       " 8749,\n",
       " 550,\n",
       " 513,\n",
       " 1456,\n",
       " 49,\n",
       " 513,\n",
       " 1683,\n",
       " 14938,\n",
       " 49,\n",
       " 723,\n",
       " 18275,\n",
       " 25166,\n",
       " 2683,\n",
       " 2584,\n",
       " 7790,\n",
       " 1598,\n",
       " 36,\n",
       " 13143,\n",
       " 495,\n",
       " 2061,\n",
       " 11137,\n",
       " 555,\n",
       " 7606,\n",
       " 359,\n",
       " 4066,\n",
       " 8688,\n",
       " 36,\n",
       " 1044,\n",
       " 5902,\n",
       " 100,\n",
       " 329,\n",
       " 406,\n",
       " 13289,\n",
       " 1424,\n",
       " 13289,\n",
       " 634,\n",
       " 1755,\n",
       " 9541,\n",
       " 8389,\n",
       " 99,\n",
       " 7983]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hong kong taiwan scrambled fighter jets dispatched frigate taiwan strait wednesday china sent sole aircraft carrier waterway taiwan official central news agency reported transit aircraft carrier liaoning came amid rising tensions taiwan china donald trump broke decades protocol speaking phone taiwan president tsai election victory tsai leads political party traditionally supported taiwan formal independence china tsai visiting central america week calls officials taiwan seeking updates liaoning transit central news agency reported citing alex huang president spokesman china decision send carrier waterway separates taiwan reflects early foreign policy challenge trump force think intended intimidate worrisome taiwan point view know going ratchet pressures tensions said bonnie glaser senior adviser asia center strategic international studies washington trump administration test resolve suspect push pretty forcefully china sent carrier conducting exercises south china taiwan strait wednesday morning taiwan response time days forces region scrambled jets response chinese military activity japan south korea deployed fighters monday actions occurred squadron chinese bombers aircraft flew waters separate japan south korea japan taiwan considered beijing chinese territory governed separately forces nationalist leader chiang fled island defeat mainland communists china views assertion taiwan separateness mainland like tsai trump affront claim sovereignty united states recognized government beijing broke formal diplomatic ties taiwan china policy wake china warned incoming president making changes policy takes office zhenmin chinese vice foreign minister said wednesday taiwan strait international waterway normal liaoning pass passage effect relations said remarks carried chinese news media mark toner state department spokesman told reporters washington response question liaoning passage strait united states problem countries sailing vessels international waters long accordance international time liaoning sailed strait passed november south china commissioned year instance carrier kept western half strait closer mainland china statement wednesday morning taiwan defense ministry said liaoning staying west strait middle urged citizens remain calm transit eastern closer taiwan viewed provocative euan graham director international security program lowy institute sydney australia said chinese traveling strait logical area fleet operations long coastline order warships based northern ports like liaoning return home southern waters pass close japanese islands transit taiwan strait geography forces binary choice said graham said important liaoning conducted passage aircraft deck conducting flight operations seen provocative passed strait aircraft hangar said liaoning commissioned built soviet hull china aircraft carrier past decades united states shown resolve defend taiwan sailing carriers taiwan strait aircraft carrier nimitz transited strait amid heightened tensions beijing conducted missile exercises waters china military highly secretive inconceivable liaoning pass contested waters approval president jinping chairman central military commission controls military chinese military media described aircraft carrier embodying plans stronger navy capable projecting force china territorial waters thursday page people liberation army daily official newspaper chinese military featured report aircraft carrier latest journey headline sailing leader attentive gaze clear tribute xiaoguang spokesman taiwan affairs office beijing said news conference wednesday liaoning passage ship scheduled training western pacific begun said relationship coming year face increasing uncertainty looming risks challenges added taiwan government independence forces seriously threatened peace stability taiwan strait accusing engaging separatist activities warning china resolutely safeguard national sovereignty territorial integrity aircraft carrier passage cluster recent acts chinese military raised hackles region month chinese warship seized underwater drone belonging united states navy miles northwest subic philippines drone returned obama administration publicly chided china seizure monday japan said sent fighter jets chinese bombers surveillance planes flew east china japan china militarily weaker japan considered area backyard said lexiong naval affairs researcher shanghai university political science telling japan conflict location future battle space decided america initiative japan think meddling afield taiwan south china'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text_of_use[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[490,\n",
       " 40611,\n",
       " 1054,\n",
       " 20801,\n",
       " 2126,\n",
       " 822,\n",
       " 2929,\n",
       " 1544,\n",
       " 5209,\n",
       " 1948,\n",
       " 2027,\n",
       " 1084,\n",
       " 497,\n",
       " 1276,\n",
       " 11,\n",
       " 19669,\n",
       " 44866,\n",
       " 24530,\n",
       " 77810,\n",
       " 2044,\n",
       " 179]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences[10] #encoding for df.text_of_use[0] ->>> bi sorun var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10524,\n",
       " 9705,\n",
       " 1039,\n",
       " 6,\n",
       " 32,\n",
       " 2,\n",
       " 268,\n",
       " 272,\n",
       " 1290,\n",
       " 2104,\n",
       " 585,\n",
       " 2,\n",
       " 153,\n",
       " 2104,\n",
       " 187,\n",
       " 4022,\n",
       " 12517,\n",
       " 5893,\n",
       " 14,\n",
       " 16736,\n",
       " 850,\n",
       " 2200,\n",
       " 656,\n",
       " 418,\n",
       " 1975,\n",
       " 548,\n",
       " 6,\n",
       " 2272,\n",
       " 1608,\n",
       " 10759,\n",
       " 59,\n",
       " 1117,\n",
       " 576,\n",
       " 315,\n",
       " 9915,\n",
       " 6,\n",
       " 700,\n",
       " 15,\n",
       " 22486,\n",
       " 625,\n",
       " 3363,\n",
       " 31,\n",
       " 9705,\n",
       " 843,\n",
       " 5644,\n",
       " 43,\n",
       " 6,\n",
       " 464,\n",
       " 4435,\n",
       " 1428,\n",
       " 162,\n",
       " 2670,\n",
       " 9705,\n",
       " 7906,\n",
       " 15962,\n",
       " 2,\n",
       " 442,\n",
       " 129,\n",
       " 10524,\n",
       " 1243,\n",
       " 323,\n",
       " 709,\n",
       " 3309,\n",
       " 589,\n",
       " 7044,\n",
       " 2706,\n",
       " 6,\n",
       " 9705,\n",
       " 1039,\n",
       " 48,\n",
       " 2028,\n",
       " 622,\n",
       " 549,\n",
       " 56041,\n",
       " 3478,\n",
       " 7051,\n",
       " 1608,\n",
       " 2644,\n",
       " 2059,\n",
       " 1820,\n",
       " 9705,\n",
       " 1468,\n",
       " 2644,\n",
       " 2059,\n",
       " 2753,\n",
       " 33,\n",
       " 4857,\n",
       " 3337,\n",
       " 10249,\n",
       " 559,\n",
       " 1859,\n",
       " 2,\n",
       " 1257,\n",
       " 323,\n",
       " 4596,\n",
       " 1339,\n",
       " 3836,\n",
       " 1396,\n",
       " 5280,\n",
       " 13181,\n",
       " 585,\n",
       " 6,\n",
       " 6185,\n",
       " 589,\n",
       " 323,\n",
       " 400,\n",
       " 5012,\n",
       " 23427,\n",
       " 593,\n",
       " 6,\n",
       " 1056,\n",
       " 11363,\n",
       " 2519,\n",
       " 24834,\n",
       " 40062,\n",
       " 861,\n",
       " 99,\n",
       " 2,\n",
       " 153,\n",
       " 171,\n",
       " 24655,\n",
       " 2722,\n",
       " 4436,\n",
       " 5980,\n",
       " 12727,\n",
       " 55,\n",
       " 743,\n",
       " 1150,\n",
       " 385,\n",
       " 171,\n",
       " 1023,\n",
       " 560,\n",
       " 488,\n",
       " 7280,\n",
       " 171,\n",
       " 1023,\n",
       " 24655,\n",
       " 2722,\n",
       " 5614,\n",
       " 2,\n",
       " 550,\n",
       " 1150,\n",
       " 422,\n",
       " 410,\n",
       " 3869,\n",
       " 662,\n",
       " 1013,\n",
       " 8091,\n",
       " 1538,\n",
       " 6692,\n",
       " 2104,\n",
       " 4471,\n",
       " 724,\n",
       " 5533,\n",
       " 2281,\n",
       " 28896,\n",
       " 2673,\n",
       " 1428,\n",
       " 8767,\n",
       " 6996,\n",
       " 4116]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = pad_sequences(train_sequences,maxlen = 40, padding = 'post', truncating = 'post')\n",
    "padded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  717,   194,  1968,  5307,   789,    14,    17,  5513,   323,\n",
       "        6787, 34640,  1364,  1690, 16338,   525,   143,  1011,   369,\n",
       "       16338,   667,  1237, 16338,  1924,  1011,   877,  3056,   659,\n",
       "       19661,  1104,  6666,  7984,  1146,   429,  5893,    38,  4953,\n",
       "          33, 25434,  1205,  3929])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for i,doc in enumerate(padded_train[:2]):\n",
    "     print(\"The padded encoding for document\",i+1,\" is : \",doc)'''\n",
    "padded_train[0] #padded single text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   70,  2914,  1511,    17,   211,   418, 13143,  2181,   470,\n",
       "        1825,   438, 11988,  3030,   457, 10540,  5566,  3297,  8481,\n",
       "         133,   211,   418,  1680,  4289,  3244, 11864, 17638,   406,\n",
       "        1230,   247, 19859, 30465, 46077,   416,   780, 12805,  2945,\n",
       "        6327, 28337, 17483,   406])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_test[0] #padded single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         19107584  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 19,403,777\n",
      "Trainable params: 19,403,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "# Embeddidng layer\n",
    "model.add(Embedding(total_words, output_dim = 128))\n",
    "\n",
    "\n",
    "# Bi-Directional RNN and LSTM\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1,activation= 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "206/206 [==============================] - 61s 279ms/step - loss: 0.4132 - acc: 0.7906 - val_loss: 0.1846 - val_acc: 0.9214\n",
      "Epoch 2/5\n",
      "206/206 [==============================] - 57s 275ms/step - loss: 0.0602 - acc: 0.9797 - val_loss: 0.2028 - val_acc: 0.9214\n",
      "Epoch 3/5\n",
      "206/206 [==============================] - 57s 277ms/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.3292 - val_acc: 0.9111\n",
      "Epoch 4/5\n",
      "206/206 [==============================] - 56s 274ms/step - loss: 0.0041 - acc: 0.9985 - val_loss: 0.3051 - val_acc: 0.9200\n",
      "Epoch 5/5\n",
      "206/206 [==============================] - 56s 272ms/step - loss: 0.0063 - acc: 0.9984 - val_loss: 0.4983 - val_acc: 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29fae6d18e0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train, y_train, batch_size = 64, validation_split = 0.1, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i in range(len(pred)):\n",
    "    if pred[i].item() > 0.95:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.9012852064533771\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(list(y_test), prediction)\n",
    "\n",
    "print(\"Model Accuracy : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1946,  143],\n",
       "       [ 218, 1350]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,prediction, labels=[0,1]) #1608 true tested text which 63 times ML was wrong "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "if os.path.isfile('models/lstm_model.h5') is False:\n",
    "    model.save('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
